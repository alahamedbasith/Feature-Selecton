{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52e69fd",
   "metadata": {},
   "source": [
    "##  Wrapper methods - Forward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8cde5",
   "metadata": {},
   "source": [
    "   Wrapper methods are a category of feature selection techniques that use the performance of a machine learning model as a criterion for selecting the most relevant features. These methods assess the quality of a subset of features by training and evaluating a model with that subset and making decisions based on the model's performance. The key idea is to search through different combinations of features to find the subset that optimizes the model's predictive performance. Common wrapper methods include:\n",
    "\n",
    "1. **Forward Selection:**\n",
    "   - In forward selection, you start with an empty set of features and iteratively add the most informative features one at a time.\n",
    "   - The process begins by evaluating the performance of the model with each individual feature separately and selecting the one that performs the best.\n",
    "   - Then, you continue to add features one by one, selecting the feature that contributes the most to the model's performance.\n",
    "   - This process continues until a predefined stopping criterion is met, such as a maximum number of features or a decrease in model performance.\n",
    "\n",
    "2. **Backward Elimination:**\n",
    "   - In backward elimination, you begin with all the available features and iteratively remove the least informative features one at a time.\n",
    "   - The process starts by evaluating the performance of the model using all the features.\n",
    "   - You then remove the feature that has the least impact on the model's performance.\n",
    "   - The process continues, removing one feature at a time, until you reach a stopping criterion.\n",
    "\n",
    "Wrapper methods have some advantages and disadvantages:\n",
    "\n",
    "Advantages:\n",
    "- They consider the interaction between features, which can be important in some cases.\n",
    "- They directly use the model's performance as a criterion, which may lead to better feature selection for specific modeling tasks.\n",
    "\n",
    "Disadvantages:\n",
    "- They can be computationally expensive, especially when the feature space is large.\n",
    "- They are model-dependent, meaning that the choice of the machine learning algorithm can affect the feature selection process.\n",
    "- The optimal feature subset selected by wrapper methods may not be the same for different machine learning models.\n",
    "\n",
    "Despite the computational cost, wrapper methods are often used in situations where predictive performance is the primary goal, and a small, informative feature subset is crucial. These methods help to find the most relevant features for a specific machine learning task by considering how features impact the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2cf05",
   "metadata": {},
   "source": [
    "## Forward Selection\n",
    "\n",
    "   - In forward selection, you start with an empty set of features and iteratively add the most informative features one at a time.\n",
    "   - The process begins by evaluating the performance of the model with each individual feature separately and selecting the one that performs the best.\n",
    "   - Then, you continue to add features one by one, selecting the feature that contributes the most to the model's performance.\n",
    "   - This process continues until a predefined stopping criterion is met, such as a maximum number of features or a decrease in model performance.\n",
    "   \n",
    "Forward feature selection is a feature selection technique that involves iteratively adding one feature at a time to the model based on its performance. Like any method, forward selection has its advantages and disadvantages:\n",
    "\n",
    "**Advantages of Forward Selection:**\n",
    "\n",
    "1. **Simplicity:** Forward selection is easy to understand and implement. It starts with an empty set of features and builds up the feature set incrementally, making it accessible to those new to feature selection techniques.\n",
    "\n",
    "2. **Efficiency:** In many cases, forward selection can be more computationally efficient compared to some other feature selection techniques because it only evaluates the performance of one feature at a time.\n",
    "\n",
    "3. **Interpretability:** The resulting feature set can be more interpretable since you can see the sequence of feature additions and understand which features contribute most to the model's performance.\n",
    "\n",
    "4. **Flexibility:** Forward selection can be used with a variety of machine learning algorithms, making it a versatile feature selection approach.\n",
    "\n",
    "**Disadvantages of Forward Selection:**\n",
    "\n",
    "1. **Not Guaranteed to Find the Best Subset:** While forward selection builds up a feature subset iteratively, it does not guarantee that it will find the best subset of features for a given problem. It may get stuck in suboptimal solutions and not explore all possible combinations.\n",
    "\n",
    "2. **Time-Consuming for Large Feature Spaces:** In cases where the feature space is large, forward selection can be computationally expensive. As it evaluates features one by one, the number of evaluations can grow significantly.\n",
    "\n",
    "3. **Dependent on the Order of Features:** The order in which features are added can significantly impact the final subset selected. If a critical feature is added late in the process, it may not be given proper consideration.\n",
    "\n",
    "4. **No Interaction Consideration:** Forward selection does not explicitly consider interactions between features. It focuses on the individual impact of each feature on the model's performance.\n",
    "\n",
    "5. **Potential Overfitting:** There's a risk of overfitting if the selection process continues until a stopping criterion is met. Overfit models may not generalize well to new data.\n",
    "\n",
    "6. **Sensitive to Noise:** Forward selection can be sensitive to noise in the data. Noisy features might be selected if they appear to improve model performance on the training data.\n",
    "\n",
    "In summary, forward selection is a straightforward and interpretable feature selection method that can work well for certain problems, especially when you have a relatively small feature space. However, it has limitations in terms of not guaranteeing the best subset and being potentially time-consuming for large feature spaces. The choice of feature selection technique should depend on the specific characteristics of your dataset and the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c3deb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4dbf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset from scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "data['species'] = iris.target_names[iris.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5350284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X = data.drop('species', axis=1)\n",
    "y = data['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffcf4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca1478e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4f4b165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1] # Select the column count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a97b542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features using Forward Selection: ['petal width (cm)']\n",
      "Best Accuracy using Forward Selection: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Forward Selection for feature selection\n",
    "selected_features = []  # Start with Empty feature list\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    best_feature = None\n",
    "    for feature in X_train.columns:\n",
    "        temp_features = selected_features + [feature]\n",
    "        classifier.fit(X_train[temp_features], y_train)\n",
    "        y_pred = classifier.predict(X_test[temp_features])\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_feature = feature\n",
    "    if best_feature is not None:\n",
    "        selected_features.append(best_feature)\n",
    "\n",
    "print(\"Selected features using Forward Selection:\", selected_features)\n",
    "print(\"Best Accuracy using Forward Selection:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66517056",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "   The Forward Selection and Backward Selection are can use any of ML algorithm not only Random Forest Classifier.It depends on the problem and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14af56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
