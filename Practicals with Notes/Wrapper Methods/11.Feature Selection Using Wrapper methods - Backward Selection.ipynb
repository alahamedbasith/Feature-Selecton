{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013e8630",
   "metadata": {},
   "source": [
    "##  Wrapper methods - Backward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbe7f6",
   "metadata": {},
   "source": [
    "Backward feature selection is another feature selection technique, which is essentially the reverse of forward selection. In backward selection, you start with all available features and iteratively remove one feature at a time, evaluating the impact on model performance. Here are the advantages and disadvantages of backward selection:\n",
    "\n",
    "**Advantages of Backward Selection:**\n",
    "\n",
    "1. **Simplicity:** Similar to forward selection, backward selection is relatively easy to understand and implement. It starts with all features and eliminates them one by one, making it accessible to users with varying levels of expertise.\n",
    "\n",
    "2. **Guaranteed Subset Size:** Backward selection allows you to specify the desired size of the feature subset. This can be useful if you want to limit the number of features for practical or interpretability reasons.\n",
    "\n",
    "3. **Model Generalization:** By iteratively removing features, backward selection can help in reducing overfitting, which is particularly valuable when working with complex models or small datasets.\n",
    "\n",
    "4. **Independence from Feature Order:** Unlike forward selection, where the order of feature addition is critical, backward selection is less sensitive to the order in which features are removed. This can make the process more stable and less dependent on specific feature sequences.\n",
    "\n",
    "**Disadvantages of Backward Selection:**\n",
    "\n",
    "1. **Not Guaranteed to Find the Best Subset:** Like forward selection, backward selection does not guarantee that it will find the optimal subset of features. It may miss important feature interactions and fail to explore all possible combinations.\n",
    "\n",
    "2. **Computational Intensity:** Backward selection can be computationally intensive for datasets with a large number of features. It requires evaluating models multiple times, which can be time-consuming.\n",
    "\n",
    "3. **Loss of Information:** Removing features may lead to a loss of potentially valuable information, especially if there are interactions or dependencies between features.\n",
    "\n",
    "4. **Over-Pruning:** If not used carefully, backward selection can lead to overly simplified models that might underperform in capturing the underlying patterns in the data.\n",
    "\n",
    "5. **Limited Interpretability:** The final subset of features selected by backward selection may not be as interpretable as those selected by forward selection, especially if it includes complex interactions or dependencies between features.\n",
    "\n",
    "In summary, backward selection can be a useful feature selection technique, especially when you want to limit the number of features and reduce overfitting. However, it shares some limitations with forward selection, such as not guaranteeing the best subset and potential computational intensity. The choice between forward and backward selection often depends on the specific characteristics of your dataset and the goals of your feature selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5835981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17668c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 20\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset from scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "data['species'] = iris.target_names[iris.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79e8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X = data.drop('species', axis=1)\n",
    "y = data['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beaf174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c128d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features using Backward Selection: ['sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Best Accuracy using Backward Selection: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Backward Selection for feature selection\n",
    "selected_features = list(X_train.columns)\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    worst_feature = None\n",
    "    for feature in selected_features:\n",
    "        temp_features = selected_features.copy()\n",
    "        temp_features.remove(feature)\n",
    "        classifier.fit(X_train[temp_features], y_train)\n",
    "        y_pred = classifier.predict(X_test[temp_features])\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            worst_feature = feature\n",
    "    if worst_feature is not None:\n",
    "        selected_features.remove(worst_feature)\n",
    "\n",
    "print(\"Selected features using Backward Selection:\", selected_features)\n",
    "print(\"Best Accuracy using Backward Selection:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b03fd",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "   The Forward Selection and Backward Selection are can use any of ML algorithm not only Random Forest Classifier.It depends on the problem and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55c640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
